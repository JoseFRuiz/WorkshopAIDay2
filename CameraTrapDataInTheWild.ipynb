{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data in Wild! Camera Traps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to\n",
    "\n",
    "1. Show how to download and interact with the datasets package on huggingface, a common place to find data.\n",
    "2. Generate a tiny dataset from the very large original dataset.\n",
    "3. Download images from a cloud bucket\n",
    "4. Create a pytorch dataset, dataloader and pythorch-lightning model\n",
    "5. Run a basic training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josefrancisco/opt/anaconda3/envs/workshopaiday2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 104780 examples [00:17, 5838.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataframe with shape: (64974, 3)\n",
      "\n",
      "First few rows:\n",
      "           filename                                              image  \\\n",
      "0  A01/01100085.JPG  https://lilablobssc.blob.core.windows.net/orin...   \n",
      "1  A01/01100087.JPG  https://lilablobssc.blob.core.windows.net/orin...   \n",
      "2  A01/01140091.JPG  https://lilablobssc.blob.core.windows.net/orin...   \n",
      "3  A01/01140092.JPG  https://lilablobssc.blob.core.windows.net/orin...   \n",
      "4  A01/01140093.JPG  https://lilablobssc.blob.core.windows.net/orin...   \n",
      "\n",
      "                 species  \n",
      "0  dasyprocta fuliginosa  \n",
      "1  dasyprocta fuliginosa  \n",
      "2          pecari tajacu  \n",
      "3          pecari tajacu  \n",
      "4          pecari tajacu  \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "matplotlib is required for plotting when the default backend \"matplotlib\" is selected.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m df = df.groupby(\u001b[33m'\u001b[39m\u001b[33mspecies\u001b[39m\u001b[33m'\u001b[39m).sample(\u001b[32m20\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Plot\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspecies\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbar\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/workshopaiday2/lib/python3.11/site-packages/pandas/plotting/_core.py:947\u001b[39m, in \u001b[36mPlotAccessor.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     plot_backend = \u001b[43m_get_plot_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     x, y, kind, kwargs = \u001b[38;5;28mself\u001b[39m._get_call_args(\n\u001b[32m    950\u001b[39m         plot_backend.\u001b[34m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m._parent, args, kwargs\n\u001b[32m    951\u001b[39m     )\n\u001b[32m    953\u001b[39m     kind = \u001b[38;5;28mself\u001b[39m._kind_aliases.get(kind, kind)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/workshopaiday2/lib/python3.11/site-packages/pandas/plotting/_core.py:1944\u001b[39m, in \u001b[36m_get_plot_backend\u001b[39m\u001b[34m(backend)\u001b[39m\n\u001b[32m   1941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend_str \u001b[38;5;129;01min\u001b[39;00m _backends:\n\u001b[32m   1942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backends[backend_str]\n\u001b[32m-> \u001b[39m\u001b[32m1944\u001b[39m module = \u001b[43m_load_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1945\u001b[39m _backends[backend_str] = module\n\u001b[32m   1946\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/workshopaiday2/lib/python3.11/site-packages/pandas/plotting/_core.py:1874\u001b[39m, in \u001b[36m_load_backend\u001b[39m\u001b[34m(backend)\u001b[39m\n\u001b[32m   1872\u001b[39m         module = importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33mpandas.plotting._matplotlib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1873\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1874\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   1875\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmatplotlib is required for plotting when the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1876\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mdefault backend \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m is selected.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1877\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n\u001b[32m   1880\u001b[39m found_backend = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"society-ethics/lila_camera_traps\", \"Orinoquia Camera Traps\", split=\"train\", trust_remote_code=True)\n",
    "taxonomy = dataset.features[\"annotations\"].feature[\"taxonomy\"]\n",
    "\n",
    "# Create empty lists to store data\n",
    "filenames = []\n",
    "species = []\n",
    "images = []\n",
    "\n",
    "# Extract filename and species for non-empty annotations\n",
    "for example in dataset:\n",
    "    species_id = example[\"annotations\"][\"taxonomy\"][0][\"species\"]\n",
    "    if species_id is not None:\n",
    "        images.append(example[\"image\"])\n",
    "        filenames.append(example[\"file_name\"])\n",
    "        species_name = taxonomy[\"species\"].int2str(species_id)\n",
    "        species.append(species_name)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'image': images,\n",
    "    'species': species\n",
    "})\n",
    "\n",
    "print(\"Created dataframe with shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# select the 20 images from the top 10 species\n",
    "df = df[df['species'].isin(df['species'].value_counts().head(10).index)]\n",
    "df = df.groupby('species').sample(20)\n",
    "\n",
    "# Plot\n",
    "df.groupby('species').size().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images: 100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_images(df, output_dir=\"images\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading images\"):\n",
    "        filename = row['filename']\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        # Recursively make directories\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(row['image'].replace(\"https://lilablobssc.blob.core.windows.net\",\"https://lilawildlife.blob.core.windows.net/lila-wildlife\"))\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img.save(output_path)\n",
    "            else:\n",
    "                print(f\"Failed to download {filename}: Status code {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "download_images(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train/test split\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "In the pytorch universe there are three key elements, datasets, dataloaders and models.\n",
    "\n",
    "Torch datasets are abstractions that handle data loading and preprocessing. They are typically subclasses of `torch.utils.data.Dataset` and define two main methods: `__len__` (returns the size of the dataset) and `__getitem__` (retrieves a data sample).\n",
    "\n",
    "Dataloaders, implemented via `torch.utils.data.DataLoader`, provide an iterable over a dataset, enabling efficient batching, shuffling, and parallel data loading.\n",
    "\n",
    "Models in PyTorch are defined as subclasses of `torch.nn.Module`. They encapsulate layers and define the forward pass of the network.\n",
    "\n",
    "PyTorch Lightning modules integrate these components by organizing the training, validation, and testing logic. A `LightningModule` defines methods like `training_step`, `validation_step`, and `test_step`, and connects datasets, dataloaders, and models into a cohesive workflow. This abstraction simplifies training loops and enables seamless integration with hardware accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_classes:  Index(['bos taurus', 'cuniculus paca', 'dasyprocta fuliginosa', 'mitu salvini',\n",
      "       'nasua nasua', 'pecari tajacu', 'penelope jacquacu',\n",
      "       'tamandua tetradactyla', 'tapirus terrestris', 'tayassu pecari'],\n",
      "      dtype='object', name='species')\n",
      "Number of train images:  150\n",
      "Number of test images:  50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train test split\n",
    "# Filter out classes with less than 5 images\n",
    "class_counts = df[\"species\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 5].index\n",
    "filtered_annotations = df[df[\"species\"].isin(valid_classes)]\n",
    "\n",
    "# Split images into train and test sets\n",
    "train_images = []\n",
    "test_images = []\n",
    "\n",
    "for common_name in valid_classes:\n",
    "    class_images = filtered_annotations[filtered_annotations[\"species\"] == common_name][\"filename\"].tolist()\n",
    "    if len(class_images) > 5:\n",
    "        train, test = train_test_split(class_images, test_size=5, random_state=42)\n",
    "        train_images.extend(train)\n",
    "        test_images.extend(test)\n",
    "\n",
    "print(\"valid_classes: \", valid_classes)\n",
    "print(\"Number of train images: \", len(train_images))\n",
    "print(\"Number of test images: \", len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/A04/102EK113/07020114.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m test_dataset = WildlifeDataset(test_images, filtered_annotations)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Show example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m image, label = \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mImage shape: \u001b[39m\u001b[33m\"\u001b[39m, image.shape)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLabel: \u001b[39m\u001b[33m\"\u001b[39m, label)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mWildlifeDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     22\u001b[39m label = \u001b[38;5;28mself\u001b[39m.string_to_label[str_label]\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Numpy array, channels first\u001b[39;00m\n\u001b[32m     28\u001b[39m image = np.array(image).transpose(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).astype(np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/workshopaiday2/lib/python3.11/site-packages/PIL/Image.py:3505\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3502\u001b[39m     filename = os.fspath(fp)\n\u001b[32m   3504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3505\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3506\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'images/A04/102EK113/07020114.JPG'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class WildlifeDataset(Dataset):\n",
    "    def __init__(self, image_paths, annotations, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        classes = annotations['species'].unique()\n",
    "        self.string_to_label = {label: i for i, label in enumerate(classes)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(\"images\", self.image_paths[idx])\n",
    "        str_label = self.annotations.loc[self.annotations['filename'] == self.image_paths[idx], 'species'].values[0]\n",
    "        label = self.string_to_label[str_label]\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Numpy array, channels first\n",
    "        image = np.array(image).transpose(2, 0, 1).astype(np.float32)\n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Example usage\n",
    "train_dataset = WildlifeDataset(train_images, filtered_annotations)\n",
    "test_dataset = WildlifeDataset(test_images, filtered_annotations)\n",
    "\n",
    "# Show example\n",
    "image, label = train_dataset[0]\n",
    "print(\"Image shape: \", image.shape)\n",
    "print(\"Label: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataloader\n",
    "\n",
    "# Create DataLoaders for train and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Example: Iterate through the train_loader\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch size: {len(images)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminología Clave de un Módulo de PyTorch Lightning\n",
    "\n",
    "### Terminología Clave\n",
    "1. **Datasets**: Abstracciones que manejan la carga y preprocesamiento de datos. Son subclases de `torch.utils.data.Dataset` y definen dos métodos principales: `__len__` (devuelve el tamaño del conjunto de datos) y `__getitem__` (recupera una muestra de datos).\n",
    "\n",
    "2. **Dataloaders**: Implementados mediante `torch.utils.data.DataLoader`, proporcionan un iterable sobre un conjunto de datos, permitiendo un batching eficiente, mezcla de datos y carga paralela.\n",
    "\n",
    "3. **Models**: Definidos como subclases de `torch.nn.Module`. Encapsulan capas y definen el paso hacia adelante (forward pass) de la red.\n",
    "\n",
    "4. **LightningModule**: Un módulo de PyTorch Lightning que organiza la lógica de entrenamiento, validación y prueba. Define métodos como `training_step`, `validation_step` y `test_step`, conectando datasets, dataloaders y modelos en un flujo de trabajo cohesivo. Esta abstracción simplifica los bucles de entrenamiento y permite una integración fluida con aceleradores de hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /blue/ewhite/b.weinstein/miniconda3/envs/megadetecto ...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/b.weinstein/AI_for_ecology_workshop/notebooks/lightning_logs/version_65324895/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | criterion | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "70        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=4` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/75 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 75/75 [00:05<00:00, 13.57it/s, v_num=6.53e+7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 75/75 [00:05<00:00, 12.59it/s, v_num=6.53e+7]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "class ResNetClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Replace the final fully connected layer to match the number of classes\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = self.accuracy(preds, labels)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNetClassifier(num_classes=len(df[\"species\"].unique()))\n",
    "\n",
    "# Create a PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model, train_dataloaders =train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /blue/ewhite/b.weinstein/miniconda3/envs/megadetecto ...\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:149: `.validate(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.validate(ckpt_path='best')` to use the best model or `.validate(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /home/b.weinstein/AI_for_ecology_workshop/notebooks/lightning_logs/version_65324895/checkpoints/epoch=9-step=750.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/b.weinstein/AI_for_ecology_workshop/notebooks/lightning_logs/version_65324895/checkpoints/epoch=9-step=750.ckpt\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/megadetector/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=4` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 25/25 [00:05<00:00,  4.98it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.1599999964237213\n",
      "        val_loss             2.180530548095703\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 2.180530548095703, 'val_acc': 0.1599999964237213}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshopaiday2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
